\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[table]{xcolor}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{tcolorbox}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning, shadows, fit, calc, backgrounds}
\usepackage{mdframed}
\usepackage{float}
\usepackage{placeins}

\geometry{margin=2.5cm}

% Define colors
\definecolor{primaryblue}{RGB}{102, 126, 234}
\definecolor{secondaryblue}{RGB}{118, 75, 162}
\definecolor{accentgreen}{RGB}{67, 233, 123}
\definecolor{accentorange}{RGB}{255, 159, 64}
\definecolor{lightgray}{RGB}{245, 245, 245}
\definecolor{darkgray}{RGB}{51, 51, 51}
\definecolor{primarypurple}{RGB}{118, 75, 162}

% Code styling
\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{primaryblue}\bfseries,
    commentstyle=\color{gray}\itshape,
    stringstyle=\color{accentorange},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    rulecolor=\color{primaryblue},
    numbers=left,
    numberstyle=\tiny\color{gray},
    backgroundcolor=\color{lightgray}
}

% Section styling
\titleformat{\section}
{\color{primaryblue}\normalfont\Large\bfseries}
{\color{primaryblue}\thesection}{1em}{}

\titleformat{\subsection}
{\color{secondaryblue}\normalfont\large\bfseries}
{\color{secondaryblue}\thesubsection}{1em}{}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\color{primaryblue}Analyse E-commerce Temps Réel}
\fancyhead[R]{\color{secondaryblue}AL AZAMI TAREK}
\fancyfoot[C]{\color{darkgray}\thepage}
\renewcommand{\headrulewidth}{2pt}
\renewcommand{\headrule}{\hbox to\headwidth{\color{primaryblue}\leaders\hrule height \headrulewidth\hfill}}

\begin{document}

% Title page
\begin{titlepage}
    \centering
    
    % Top decoration
    \begin{tikzpicture}[remember picture,overlay]
        \fill[primaryblue] (current page.north west) rectangle ([yshift=-3cm]current page.north east);
        \fill[secondaryblue] ([yshift=-3cm]current page.north west) rectangle ([yshift=-3.5cm]current page.north east);
    \end{tikzpicture}
    
    \vspace{1cm}
    
    {\color{white}\LARGE \textbf{École Nationale des Sciences Appliquées}}\\[0.3cm]
    {\color{white}\large ENSA}\\[3cm]
    
    \begin{tcolorbox}[colback=primaryblue!10,colframe=primaryblue,width=0.9\textwidth,arc=5mm]
        \centering
        {\Huge \textbf{\color{primaryblue}Plateforme d'Analyse}}\\[0.3cm]
        {\Huge \textbf{\color{secondaryblue}E-commerce}}\\[0.3cm]
        {\Huge \textbf{\color{primaryblue}en Temps Réel}}\\[0.5cm]
        {\Large \color{darkgray}Projet Big Data}
    \end{tcolorbox}
    
    \vspace{2cm}
    
    \begin{minipage}{0.45\textwidth}
        \begin{flushleft}
            \textbf{\color{primaryblue}Réalisé par :}\\
            {\large AL AZAMI TAREK}
        \end{flushleft}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \begin{flushright}
            \textbf{\color{secondaryblue}Encadré par :}\\
            {\large Professeur Hassan BADIR}
        \end{flushright}
    \end{minipage}
    
    \vfill
    
    {\large \color{darkgray}Année Universitaire 2025-2026}
    
\end{titlepage}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Contexte}
Dans l'ère du commerce électronique, les entreprises génèrent des millions de transactions quotidiennes. L'analyse en temps réel de ces données est devenue cruciale pour :

\begin{itemize}[leftmargin=*]
    \item[\color{accentgreen}$\bullet$] Comprendre le comportement des clients
    \item[\color{accentgreen}$\bullet$] Optimiser les stratégies commerciales
    \item[\color{accentgreen}$\bullet$] Prendre des décisions data-driven
    \item[\color{accentgreen}$\bullet$] Détecter les tendances en temps réel
\end{itemize}

\subsection{Objectifs du Projet}

\begin{tcolorbox}[colback=primaryblue!5,colframe=primaryblue,title=\textbf{Objectifs Principaux}]
\begin{enumerate}[leftmargin=*]
    \item \textbf{Pipeline temps réel} : Ingestion et traitement de transactions e-commerce
    \item \textbf{Analyse streaming} : Traitement avec Apache Spark Streaming
    \item \textbf{Stockage optimisé} : Format Parquet pour performances maximales
    \item \textbf{Visualisation} : Dashboard interactif avec mise à jour automatique
    \item \textbf{Automatisation} : Déploiement Docker simplifié
\end{enumerate}
\end{tcolorbox}

\subsection{Problématique}
\begin{mdframed}[backgroundcolor=accentorange!10,linecolor=accentorange,linewidth=2pt]
\textit{Comment concevoir une architecture Big Data capable de traiter des milliers de transactions par seconde tout en fournissant des analyses et visualisations en temps réel ?}
\end{mdframed}

\newpage

\section{Architecture Technique}

\subsection{Vue d'Ensemble}

\begin{figure}[h]
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[
    node distance=1cm and 1cm,
    font=\small\sffamily,
    container/.style={
        rectangle, 
        draw=secondaryblue, 
        thick, 
        fill=white, 
        rounded corners, 
        minimum width=2.5cm, 
        minimum height=1.5cm, 
        align=center,
        drop shadow
    },
    volume/.style={
        cylinder, 
        shape border rotate=90, 
        draw=accentorange, 
        thick, 
        fill=accentorange!10, 
        aspect=0.25, 
        minimum width=2cm, 
        minimum height=1.5cm, 
        align=center
    },
    arrow/.style={
        draw=darkgray, 
        -latex, 
        thick,
        rounded corners
    }
]

% Network Box
\node[draw=lightgray, dashed, fill=lightgray!20, fit={(0,0) (14,8)}, inner sep=0.5cm, label={[anchor=north west]north west:\textcolor{gray}{\textbf{Docker Network: bigdata-network}}}] (network) {};

% Containers
\node[container, fill=accentgreen!10] (producer) at (1.5, 6) {\textbf{Producer}\\Python};
\node[container, fill=primaryblue!10] (kafka) at (5, 6) {\textbf{Kafka}\\Broker};
\node[container, fill=secondaryblue!10] (zk) at (5, 4) {\textbf{Zookeeper}\\Coordination};

\node[container, fill=accentorange!10] (spark) at (9, 6) {\textbf{Spark}\\Master/Worker};

\node[container, fill=secondaryblue!10] (dashboard) at (13, 6) {\textbf{Dashboard}\\Flask App};

% Volumes/Storage
\node[volume, below=of spark] (storage) {\textbf{Shared Volume}\\(Parquet)};
\node[volume, below=of kafka, xshift=-2cm] (hdfs) {\textbf{HDFS}\\DataNodes};

% Connections
\draw[arrow] (producer) -- node[above, font=\tiny]{JSON} (kafka);
\draw[arrow] (kafka) -- (spark);
\draw[arrow] (zk) -- (kafka);
\draw[arrow] (spark) -- node[right, font=\tiny]{Write} (storage);
\draw[arrow] (storage) -| node[near start, below, font=\tiny]{Read} (dashboard);

\end{tikzpicture}
}
\caption{Architecture Docker du Projet}
\end{figure}
\newpage
\subsection{Composants Principaux}

\subsubsection{\color{primaryblue}Apache Kafka - Message Broker}
\begin{itemize}[leftmargin=*]
    \item \textbf{Version} : Confluent 7.5.0
    \item \textbf{Topic} : \texttt{ecommerce-transactions}
    \item \textbf{Partitions} : 3
    \item \textbf{Rôle} : Ingestion fiable des transactions avec latence minimale
\end{itemize}

\subsubsection{\color{primaryblue}Apache Spark - Traitement Distribué}
\begin{itemize}[leftmargin=*]
    \item \textbf{Version} : 3.5.0
    \item \textbf{Architecture} : 1 Master + 1 Worker
    \item \textbf{Mémoire} : 2 GB par worker
    \item \textbf{Fonctions} :
    \begin{itemize}
        \item Consommation messages Kafka
        \item Agrégations temps réel (fenêtres 1 minute)
        \item Écriture Parquet optimisée
    \end{itemize}
\end{itemize}

\subsubsection{\color{primaryblue}Dashboard Flask - Visualisation}
\begin{itemize}[leftmargin=*]
    \item \textbf{Backend} : Flask + Pandas + PyArrow
    \item \textbf{Frontend} : HTML5 + JavaScript + Chart.js
    \item \textbf{Features} :
    \begin{itemize}
        \item Métriques temps réel
        \item Graphiques interactifs
        \item Auto-refresh 10 secondes
    \end{itemize}
\end{itemize}

\subsubsection{\color{primaryblue}Hadoop HDFS - Stockage}
\begin{itemize}[leftmargin=*]
    \item \textbf{Version} : 3.2.1
    \item \textbf{Composants} : NameNode + DataNode
    \item \textbf{Rôle} : Stockage distribué et fiable
\end{itemize}

\newpage

\section{Modèle de Données}

\subsection{Schéma des Transactions}

\begin{tcolorbox}[
  colback=lightgray,
  colframe=primaryblue,
  width=\textwidth,
  boxrule=0.8pt,
  arc=4pt,
  left=6pt,
  right=6pt,
  top=6pt,
  bottom=6pt
]
\centering
\small
\begin{tabular}{|l|l|p{6cm}|}
\hline
\rowcolor{primaryblue!20}
\textbf{Champ} & \textbf{Type} & \textbf{Description} \\
\hline
transaction\_id & String & UUID unique \\
\hline
user\_id & String & Identifiant utilisateur \\
\hline
city & String & Ville (6 villes marocaines) \\
\hline
category & String & Catégorie produit (6 types) \\
\hline
amount & Double & Montant (10--500 €) \\
\hline
quantity & Integer & Quantité (1--5) \\
\hline
payment\_method & String & Méthode de paiement (4 types) \\
\hline
timestamp & Timestamp & Date et heure \\
\hline
\end{tabular}
\end{tcolorbox}

\subsection{Format de Stockage : Parquet}

\begin{tcolorbox}[colback=accentgreen!10,colframe=accentgreen,title=\textbf{Avantages du Format Parquet}]
\begin{itemize}[leftmargin=*]
    \item[\color{accentgreen}✓] \textbf{Compression} : Réduction de 75\% de l'espace disque
    \item[\color{accentgreen}✓] \textbf{Performance} : Lecture columnaire 10x plus rapide
    \item[\color{accentgreen}✓] \textbf{Schéma intégré} : Métadonnées incluses
    \item[\color{accentgreen}✓] \textbf{Compatible} : Spark, Pandas, PyArrow
\end{itemize}
\end{tcolorbox}

\newpage

\section{Flux de Données}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    auto,
    process/.style={
        rectangle,
        draw=primaryblue,
        thick,
        fill=primaryblue!10,
        text width=3cm,
        align=center,
        rounded corners,
        minimum height=1.2cm,
        drop shadow
    },
    arrow/.style={
        draw=secondaryblue,
        -latex,
        ultra thick
    }
]

% Nodes
\node[process] (gen) {1. Génération\\(Python)};
\node[process, right=of gen] (ingest) {2. Ingestion\\(Kafka)};
\node[process, right=of ingest] (proc) {3. Traitement\\(Spark Streaming)};
\node[process, below=of proc] (store) {4. Stockage\\(Parquet)};
\node[process, left=of store] (aggr) {5. Agrégation\\(Pandas)};
\node[process, left=of aggr] (viz) {6. Visualisation\\(Chart.js)};

% Edges
\draw[arrow] (gen) -- (ingest);
\draw[arrow] (ingest) -- (proc);
\draw[arrow] (proc) -- (store);
\draw[arrow] (store) -- (aggr);
\draw[arrow] (aggr) -- (viz);

\end{tikzpicture}
\caption{Flux de Données E-commerce}
\end{figure}

\subsection{Génération des Transactions}

\begin{lstlisting}[language=Python, caption={\color{primaryblue}Producteur Kafka}]
from kafka import KafkaProducer
import json, random, uuid

producer = KafkaProducer(
    bootstrap_servers=['kafka:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

while True:
    data = {
        'transaction_id': str(uuid.uuid4()),
        'user_id': f'USER_{random.randint(100, 999)}',
        'city': random.choice(cities),
        'category': random.choice(categories),
        'amount': round(random.uniform(10.0, 500.0), 2),
        'quantity': random.randint(1, 5),
        'payment_method': random.choice(payment_methods),
        'timestamp': datetime.now().isoformat()
    }
    producer.send('ecommerce-transactions', value=data)
    time.sleep(1)  # 1 transaction/seconde
\end{lstlisting}
\newpage
\subsection{Traitement Spark Streaming}

\begin{lstlisting}[language=Python, caption={\color{primaryblue}Consumer Spark}]
# Lecture depuis Kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "ecommerce-transactions") \
    .load()

# Agregations par fenetre temporelle
aggregated_df = parsed_df \
    .groupBy(window(col("timestamp"), "1 minute"), "category") \
    .agg(
        sum("amount").alias("total_revenue"),
        count("*").alias("num_transactions")
    )

# Sauvegarde Parquet
query = parsed_df.writeStream \
    .format("parquet") \
    .option("path", "/tmp/ecommerce-data/raw") \
    .trigger(processingTime='30 seconds') \
    .start()
\end{lstlisting}

\subsection{Visualisation Dashboard}

Le dashboard Flask expose des API REST :
\begin{itemize}[leftmargin=*]
    \item \texttt{/api/stats} - Métriques globales
    \item \texttt{/api/categories} - Analyse par catégorie
    \item \texttt{/api/cities} - Analyse géographique
    \item \texttt{/api/payments} - Méthodes de paiement
\end{itemize}

Le frontend JavaScript interroge ces API toutes les 10 secondes et met à jour les graphiques Chart.js en temps réel.

\newpage

\section{Déploiement Docker}

\subsection{Architecture Containerisée}

\begin{tcolorbox}[colback=secondaryblue!10,colframe=secondaryblue,title=\textbf{7 Conteneurs Docker}]
\begin{enumerate}[leftmargin=*]
    \item \textbf{Zookeeper} - Coordination Kafka
    \item \textbf{Kafka} - Message broker
    \item \textbf{Spark Master} - Nœud maître
    \item \textbf{Spark Worker} - Nœud de calcul
    \item \textbf{HDFS NameNode} - Métadonnées
    \item \textbf{HDFS DataNode} - Stockage
    \item \textbf{Dashboard} - Interface web
\end{enumerate}
\end{tcolorbox}

\subsection{Volumes Partagés}

\begin{itemize}[leftmargin=*]
    \item \textbf{ecommerce\_data} : Fichiers Parquet partagés (Spark ↔ Dashboard)
    \item \textbf{hadoop\_namenode} : Métadonnées HDFS
    \item \textbf{hadoop\_datanode} : Données HDFS
\end{itemize}

\subsection{Scripts d'Automatisation}

\begin{tcolorbox}[colback=accentgreen!10,colframe=accentgreen]
\textbf{setup.sh} : Installation complète (conteneurs + dépendances + topic Kafka)

\textbf{start-all.sh} : Démarrage automatique (dashboard + consumer + producer)

\textbf{run-analysis.sh} : Génération de rapports analytiques

\textbf{stop-all.sh} : Arrêt de tous les services
\end{tcolorbox}

\newpage

\section{Résultats}

\subsection{Exemple de Résultats}

\begin{tcolorbox}[colback=primaryblue!5,colframe=primaryblue,title=\textbf{Après 3 minutes d'exécution}]
\begin{itemize}[leftmargin=*]
    \item \textbf{Transactions analysées} : 180
    \item \textbf{Utilisateurs uniques} : 124
    \item \textbf{Chiffre d'affaires} : 36,129.69 €
    \item \textbf{Montant moyen} : 259.93 €
    \item \textbf{Articles vendus} : 450
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=secondaryblue!5,colframe=secondaryblue,title=\textbf{Top 3 Catégories}]
\begin{enumerate}[leftmargin=*]
    \item \textbf{Electronics} : 8,500 € (35 transactions)
    \item \textbf{Fashion} : 7,200 € (28 transactions)
    \item \textbf{Home \& Garden} : 6,800 € (32 transactions)
\end{enumerate}
\end{tcolorbox}

\subsection{Dashboard}

Le tableau de bord fournit :
\begin{itemize}[leftmargin=*]
    \item[\color{accentgreen}$\bullet$] 4 cartes métriques (revenus, transactions, utilisateurs, moyenne)
    \item[\color{accentgreen}$\bullet$] Graphique barres : Revenus par catégorie
    \item[\color{accentgreen}$\bullet$] Graphique circulaire : Revenus par ville
    \item[\color{accentgreen}$\bullet$] Graphique donut : Méthodes de paiement
    \item[\color{accentgreen}$\bullet$] Tableau : Top 10 transactions
\end{itemize}

\FloatBarrier % ⬅ ensures figures start here

% ===== FIGURE 1 =====
\begin{figure}[H]
\centering
\begin{tcolorbox}[
  colback=white,
  colframe=primarypurple,
  width=\textwidth,
  arc=6pt
]
\includegraphics[width=0.95\textwidth]{1.png}
\end{tcolorbox}
\caption{Interface du Dashboard E-commerce}
\end{figure}

\vspace{0.5cm}

% ===== FIGURE 2 =====
\begin{figure}[H]
\centering
\begin{tcolorbox}[
  colback=white,
  colframe=primarypurple,
  width=\textwidth,
  arc=6pt
]
\includegraphics[width=0.95\textwidth]{2.png}
\end{tcolorbox}
\caption{Visualisations des Données}
\end{figure}

\FloatBarrier
\newpage
\section{Défis Techniques}

\subsection{Permissions Docker}

\begin{mdframed}[backgroundcolor=accentorange!10,linecolor=accentorange,linewidth=2pt]
\textbf{Problème :} Spark ne pouvait pas écrire dans le volume partagé

\textbf{Solution :} Création du répertoire avec permissions appropriées
\begin{lstlisting}[language=bash]
docker exec -u root spark-master mkdir -p /tmp/ecommerce-data/raw
docker exec -u root spark-master chmod -R 777 /tmp/ecommerce-data
\end{lstlisting}
\end{mdframed}

\subsection{Partage de Données}

\begin{mdframed}[backgroundcolor=accentorange!10,linecolor=accentorange,linewidth=2pt]
\textbf{Problème :} Dashboard ne pouvait pas accéder aux fichiers Parquet de Spark

\textbf{Solution :} Volume Docker partagé \texttt{ecommerce\_data} monté sur les deux conteneurs
\end{mdframed}

\subsection{Optimisation Dashboard}

\begin{mdframed}[backgroundcolor=accentorange!10,linecolor=accentorange,linewidth=2pt]
\textbf{Problème :} PySpark trop lourd pour le conteneur dashboard (nécessite Java)

\textbf{Solution :} Utilisation de Pandas + PyArrow pour lecture efficace des Parquet
\end{mdframed}

\newpage

\section{Conclusion}

\subsection{Synthèse}
Ce projet a permis de concevoir et implémenter une plateforme complète d'analyse Big Data pour le traitement en temps réel de transactions e-commerce. L'architecture démontre la puissance des technologies modernes pour gérer des flux de données massifs.

\subsection{Compétences Acquises}

\begin{tcolorbox}[colback=primaryblue!5,colframe=primaryblue,title=\textbf{Compétences Techniques}]
\begin{itemize}[leftmargin=*]
    \item Maîtrise de l'écosystème Apache (Kafka, Spark, Hadoop)
    \item Développement d'applications streaming temps réel
    \item Containerisation et orchestration Docker
    \item Développement web full-stack (Flask + JavaScript)
    \item Traitement de données (Pandas, PySpark)
    \item Automatisation avec scripts Bash
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=secondaryblue!5,colframe=secondaryblue,title=\textbf{Compétences Architecturales}]
\begin{itemize}[leftmargin=*]
    \item Conception d'architectures Big Data scalables
    \item Patterns de traitement streaming et batch
    \item Optimisation des performances
    \item Résolution de problèmes de production
\end{itemize}
\end{tcolorbox}

\subsection{Résultats Atteints}

\begin{center}
\begin{tikzpicture}
    \foreach \i/\text in {1/Pipeline fonctionnel,2/Latence < 15s,3/Dashboard interactif,4/Automatisation complète,5/Documentation exhaustive} {
        \node[fill=accentgreen,text=white,rounded corners,minimum width=3cm,minimum height=0.8cm] at (0,-\i*0.9) {\small ✓ \text};
    }
\end{tikzpicture}
\end{center}

\subsection{Perspectives}
Ce projet constitue une base solide pour des applications Big Data en production. L'expérience acquise est directement applicable dans l'industrie, où les architectures de streaming et l'analyse temps réel sont essentielles pour la compétitivité.

\newpage

\section*{Annexes}

\subsection*{Structure du Projet}

\begin{tcolorbox}[colback=white,colframe=darkgray,title=\textbf{Arborescence des Fichiers}]
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, backgroundcolor=\color{white}]
bigdata-project/
├── docker-compose.yml          # Configuration des services
├── README.md                   # Documentation generale
├── requirements.txt            # Dependances Python
├── dashboard/
│   └── dashboard.html          # Interface Frontend
├── scripts/
│   ├── producer.py             # Generateur de transactions
│   ├── consumer_spark.py       # Traitement Spark Streaming
│   ├── analysis.py             # Analyse Batch & Reporting
│   ├── dashboard_server.py     # Backend Flask API
│   └── automation/             # Scripts d'automatisation
│       ├── setup.sh            # Installation
│       ├── start-all.sh        # Demarrage global
│       ├── run-analysis.sh     # Generation de rapports
│       └── stop-all.sh         # Arret des services
└── data/                       # Volume partage (Parquet files)
\end{lstlisting}
\end{tcolorbox}

\subsection*{Technologies Utilisées}

\begin{table}[h]
\centering
\begin{tcolorbox}[colback=lightgray,colframe=primaryblue,width=0.8\textwidth]
\begin{tabular}{|l|l|}
\hline
\rowcolor{primaryblue!20}
\textbf{Technologie} & \textbf{Version} \\
\hline
Apache Kafka & 7.5.0 \\
Apache Spark & 3.5.0 \\
Apache Hadoop & 3.2.1 \\
Python & 3.9 \\
Flask & 3.0.0 \\
Pandas & 2.1.4 \\
Chart.js & 4.4.0 \\
Docker & 20.10+ \\
\hline
\end{tabular}
\end{tcolorbox}
\end{table}

\subsection*{Commandes Essentielles}

\begin{tcolorbox}[colback=accentgreen!10,colframe=accentgreen,title=\textbf{Démarrage}]
\begin{lstlisting}[language=bash]
./scripts/automation/setup.sh
./scripts/automation/start-all.sh
\end{lstlisting}
\end{tcolorbox}

\begin{tcolorbox}[colback=primaryblue!10,colframe=primaryblue,title=\textbf{Interfaces Web}]
\begin{itemize}[leftmargin=*]
    \item Dashboard : \url{http://localhost:5000}
    \item Spark Master : \url{http://localhost:8080}
    \item HDFS : \url{http://localhost:9870}
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=accentorange!10,colframe=accentorange,title=\textbf{Arrêt}]
\begin{lstlisting}[language=bash]
./scripts/automation/stop-all.sh
\end{lstlisting}
\end{tcolorbox}

\end{document}

\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{tcolorbox}
\usepackage{enumitem}

\geometry{margin=2.5cm}

% Code styling
\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Plateforme d'Analyse E-commerce en Temps Réel}
\fancyhead[R]{AL AZAMI TAREK}
\fancyfoot[C]{\thepage}

\begin{document}

% Title page
\begin{titlepage}
    \centering
    \vspace{2cm}
    
    {\LARGE \textbf{École Nationale des Sciences Appliquées}}\\[0.5cm]
    {\large ENSA}\\[2cm]
    
    \rule{\linewidth}{0.5mm}\\[0.4cm]
    {\huge \textbf{Plateforme d'Analyse des Transactions E-commerce en Temps Réel}}\\[0.2cm]
    {\Large Projet Big Data}\\[0.4cm]
    \rule{\linewidth}{0.5mm}\\[2cm]
    
    \begin{minipage}{0.4\textwidth}
        \begin{flushleft}
            \textbf{Réalisé par :}\\
            AL AZAMI TAREK
        \end{flushleft}
    \end{minipage}
    \begin{minipage}{0.4\textwidth}
        \begin{flushright}
            \textbf{Encadré par :}\\
            Professeur Hassan BADIR
        \end{flushright}
    \end{minipage}
    
    \vfill
    
    {\large Année Universitaire 2025-2026}
    
\end{titlepage}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Contexte du Projet}
Dans le contexte actuel du commerce électronique, les entreprises génèrent des volumes massifs de données transactionnelles en temps réel. L'analyse de ces données devient cruciale pour comprendre le comportement des clients, optimiser les stratégies de vente et prendre des décisions commerciales éclairées.

Ce projet vise à développer une plateforme complète d'analyse de données Big Data pour le traitement et la visualisation en temps réel des transactions e-commerce. La solution implémente un pipeline de données moderne utilisant les technologies Apache Kafka, Apache Spark Streaming et un tableau de bord interactif.

\subsection{Objectifs}
Les objectifs principaux de ce projet sont :

\begin{itemize}
    \item Mettre en place un pipeline de données en temps réel pour l'ingestion de transactions e-commerce
    \item Traiter et analyser les flux de données avec Apache Spark Streaming
    \item Stocker les données de manière efficace au format Parquet
    \item Développer un tableau de bord interactif pour la visualisation en temps réel
    \item Automatiser le déploiement et l'exécution de la plateforme avec Docker
    \item Générer des rapports analytiques détaillés
\end{itemize}

\subsection{Problématique}
Comment concevoir et implémenter une architecture Big Data scalable capable de traiter des milliers de transactions e-commerce par seconde, tout en fournissant des analyses et visualisations en temps réel ?

\newpage

\section{Architecture Technique}

\subsection{Vue d'Ensemble}
L'architecture de la plateforme suit le modèle Lambda simplifié, combinant traitement en temps réel (streaming) et traitement batch. Le système est entièrement containerisé avec Docker pour faciliter le déploiement et la scalabilité.

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Pipeline de Données]
\textbf{Producteur Python} $\rightarrow$ \textbf{Apache Kafka} $\rightarrow$ \textbf{Spark Streaming} $\rightarrow$ \textbf{Parquet Files} $\rightarrow$ \textbf{Dashboard Flask}
\end{tcolorbox}

\subsection{Composants du Système}

\subsubsection{Apache Kafka}
\textbf{Rôle :} Message broker pour l'ingestion de données en temps réel

\textbf{Configuration :}
\begin{itemize}
    \item Version : Confluent Platform 7.5.0
    \item Topic : \texttt{ecommerce-transactions}
    \item Partitions : 3
    \item Facteur de réplication : 1
    \item Port : 9092
\end{itemize}

Kafka assure la collecte fiable des transactions avec une latence minimale et garantit la durabilité des messages.

\subsubsection{Apache Spark}
\textbf{Rôle :} Moteur de traitement distribué pour le streaming et l'analyse batch

\textbf{Configuration :}
\begin{itemize}
    \item Version : Apache Spark 3.5.0
    \item Mode : Cluster (1 Master + 1 Worker)
    \item Mémoire Worker : 2 GB
    \item Cœurs Worker : 2
    \item Ports : 8080 (Master UI), 7077 (Master), 4040 (Application UI)
\end{itemize}

\textbf{Fonctionnalités :}
\begin{enumerate}
    \item \textbf{Spark Streaming} : Consommation et traitement des messages Kafka
    \item \textbf{Agrégations en temps réel} : Calcul du chiffre d'affaires par catégorie toutes les 30 secondes
    \item \textbf{Stockage Parquet} : Écriture des données au format Parquet compressé
    \item \textbf{Analyse Batch} : Génération de rapports statistiques détaillés
\end{enumerate}

\subsubsection{Apache Hadoop HDFS}
\textbf{Rôle :} Système de fichiers distribué pour le stockage persistant

\textbf{Configuration :}
\begin{itemize}
    \item Version : Hadoop 3.2.1
    \item Composants : NameNode + DataNode
    \item Port NameNode : 9870 (UI), 9000 (RPC)
\end{itemize}

HDFS fournit un stockage fiable et scalable pour les données historiques.

\subsubsection{Dashboard Flask}
\textbf{Rôle :} Interface web pour la visualisation en temps réel

\textbf{Technologies :}
\begin{itemize}
    \item Backend : Flask (Python 3.9)
    \item Traitement de données : Pandas, PyArrow
    \item Frontend : HTML5, CSS3, JavaScript
    \item Visualisation : Chart.js 4.4.0
    \item Port : 5000
\end{itemize}

\textbf{Fonctionnalités :}
\begin{itemize}
    \item Métriques en temps réel (revenus, transactions, utilisateurs)
    \item Graphiques interactifs (barres, camembert, donut)
    \item Auto-rafraîchissement toutes les 10 secondes
    \item Design moderne avec glassmorphism
\end{itemize}

\subsubsection{Apache Zookeeper}
\textbf{Rôle :} Service de coordination pour Kafka

\textbf{Configuration :}
\begin{itemize}
    \item Version : Confluent Platform 7.5.0
    \item Port : 2181
\end{itemize}

\newpage

\section{Modèle de Données}

\subsection{Schéma des Transactions}
Chaque transaction e-commerce contient les champs suivants :

\begin{table}[h]
\centering
\begin{tabular}{|l|l|p{6cm}|}
\hline
\textbf{Champ} & \textbf{Type} & \textbf{Description} \\
\hline
transaction\_id & String & Identifiant unique (UUID) \\
user\_id & String & Identifiant utilisateur (USER\_XXX) \\
city & String & Ville (Casablanca, Rabat, Marrakech, Fès, Tanger, Agadir) \\
category & String & Catégorie produit (Electronics, Fashion, Home \& Garden, Sports, Books, Toys) \\
amount & Double & Montant en euros (10.0 - 500.0) \\
quantity & Integer & Quantité d'articles (1-5) \\
payment\_method & String & Méthode de paiement (Credit Card, PayPal, Wire Transfer, Cryptocurrency) \\
timestamp & Timestamp & Date et heure de la transaction \\
\hline
\end{tabular}
\caption{Schéma de données des transactions}
\end{table}

\subsection{Format de Stockage}
Les données sont stockées au format \textbf{Apache Parquet} avec les avantages suivants :

\begin{itemize}
    \item \textbf{Compression efficace} : Réduction de 70-80\% de l'espace disque
    \item \textbf{Lecture columnaire} : Performances optimales pour les requêtes analytiques
    \item \textbf{Schéma intégré} : Métadonnées incluses dans le fichier
    \item \textbf{Compatible} : Lecture par Spark, Pandas, PyArrow
\end{itemize}

\subsection{Volume de Données}
\begin{itemize}
    \item \textbf{Taux de génération} : 1 transaction/seconde
    \item \textbf{Volume journalier} : ~86,400 transactions
    \item \textbf{Taille moyenne} : ~200 bytes/transaction
    \item \textbf{Stockage journalier} : ~17 MB (non compressé), ~3-5 MB (Parquet)
\end{itemize}

\newpage

\section{Flux de Données}

\subsection{Génération des Transactions}
Le producteur Python (\texttt{producer.py}) simule un système e-commerce réel :

\begin{lstlisting}[language=Python, caption=Extrait du producteur Kafka]
from kafka import KafkaProducer
import json, random, uuid
from datetime import datetime

producer = KafkaProducer(
    bootstrap_servers=['kafka:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

while True:
    data = {
        'transaction_id': str(uuid.uuid4()),
        'user_id': f'USER_{random.randint(100, 999)}',
        'city': random.choice(cities),
        'category': random.choice(categories),
        'amount': round(random.uniform(10.0, 500.0), 2),
        'quantity': random.randint(1, 5),
        'payment_method': random.choice(payment_methods),
        'timestamp': datetime.now().isoformat()
    }
    producer.send('ecommerce-transactions', value=data)
    time.sleep(1)
\end{lstlisting}

\subsection{Traitement Streaming}
Spark Streaming consomme les messages Kafka et effectue :

\begin{enumerate}
    \item \textbf{Lecture du stream} : Connexion au topic Kafka
    \item \textbf{Parsing JSON} : Conversion en DataFrame Spark
    \item \textbf{Agrégations} : Calculs par fenêtres temporelles (1 minute)
    \item \textbf{Affichage console} : Visualisation des données brutes et agrégées
    \item \textbf{Écriture Parquet} : Sauvegarde persistante toutes les 30 secondes
\end{enumerate}

\begin{lstlisting}[language=Python, caption=Configuration Spark Streaming]
# Lecture depuis Kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "ecommerce-transactions") \
    .option("startingOffsets", "latest") \
    .load()

# Agregations par fenetre temporelle
aggregated_df = parsed_df_with_watermark \
    .groupBy(window(col("timestamp"), "1 minute"), "category") \
    .agg(
        sum("amount").alias("total_revenue"),
        avg("amount").alias("avg_transaction"),
        count("transaction_id").alias("num_transactions")
    )

# Sauvegarde Parquet
query_hdfs = parsed_df.writeStream \
    .outputMode("append") \
    .format("parquet") \
    .option("path", "/tmp/ecommerce-data/raw") \
    .option("checkpointLocation", "/tmp/ecommerce-data/checkpoints") \
    .trigger(processingTime='30 seconds') \
    .start()
\end{lstlisting}

\subsection{Analyse Batch}
Le script \texttt{analysis.py} effectue une analyse complète des données stockées :

\begin{itemize}
    \item \textbf{Statistiques globales} : Revenus totaux, moyennes, min/max
    \item \textbf{Analyse par catégorie} : Revenus et transactions par type de produit
    \item \textbf{Analyse géographique} : Performance par ville
    \item \textbf{Méthodes de paiement} : Distribution des préférences
    \item \textbf{Top utilisateurs} : Classement par dépenses
    \item \textbf{Top transactions} : Transactions les plus importantes
\end{itemize}

\subsection{Visualisation}
Le dashboard Flask lit les fichiers Parquet avec Pandas et expose des API REST :

\begin{itemize}
    \item \texttt{GET /api/stats} : Métriques globales
    \item \texttt{GET /api/categories} : Données par catégorie
    \item \texttt{GET /api/cities} : Données par ville
    \item \texttt{GET /api/payments} : Données par méthode de paiement
    \item \texttt{GET /api/top-transactions} : Top 10 transactions
\end{itemize}

Le frontend JavaScript interroge ces API toutes les 10 secondes et met à jour les graphiques Chart.js.

\newpage

\section{Déploiement et Infrastructure}

\subsection{Containerisation Docker}
L'ensemble de la plateforme est déployé avec Docker Compose, garantissant :

\begin{itemize}
    \item \textbf{Isolation} : Chaque service dans son propre conteneur
    \item \textbf{Reproductibilité} : Environnement identique sur toutes les machines
    \item \textbf{Scalabilité} : Ajout facile de workers Spark
    \item \textbf{Portabilité} : Déploiement sur n'importe quel système avec Docker
\end{itemize}

\subsection{Architecture Réseau}
Tous les conteneurs sont connectés au réseau \texttt{bigdata-network} (bridge) permettant :

\begin{itemize}
    \item Communication inter-conteneurs par nom de service
    \item Isolation du réseau externe
    \item Exposition sélective des ports (8080, 9092, 5000, etc.)
\end{itemize}

\subsection{Volumes Persistants}
Trois volumes Docker assurent la persistance des données :

\begin{enumerate}
    \item \textbf{hadoop\_namenode} : Métadonnées HDFS
    \item \textbf{hadoop\_datanode} : Blocs de données HDFS
    \item \textbf{ecommerce\_data} : Fichiers Parquet partagés (Spark ↔ Dashboard)
\end{enumerate}

\subsection{Scripts d'Automatisation}
Des scripts Bash facilitent l'utilisation de la plateforme :

\begin{table}[h]
\centering
\begin{tabular}{|l|p{8cm}|}
\hline
\textbf{Script} & \textbf{Fonction} \\
\hline
setup.sh & Installation initiale : démarrage conteneurs, installation dépendances, création topic Kafka \\
start-all.sh & Démarrage complet : dashboard, consumer, producer \\
start-producer.sh & Démarrage du générateur de transactions \\
start-consumer.sh & Démarrage du consumer Spark Streaming \\
run-analysis.sh & Exécution de l'analyse batch et génération du rapport \\
stop-all.sh & Arrêt de tous les services \\
\hline
\end{tabular}
\caption{Scripts d'automatisation}
\end{table}

\newpage

\section{Résultats et Analyses}

\subsection{Métriques de Performance}

\subsubsection{Latence}
\begin{itemize}
    \item \textbf{Producteur → Kafka} : < 5 ms
    \item \textbf{Kafka → Spark} : < 100 ms
    \item \textbf{Traitement Spark} : ~1-2 secondes (par micro-batch)
    \item \textbf{Écriture Parquet} : ~500 ms
    \item \textbf{Dashboard refresh} : 10 secondes
    \item \textbf{Latence end-to-end} : < 15 secondes
\end{itemize}

\subsubsection{Throughput}
\begin{itemize}
    \item \textbf{Transactions/seconde} : 1 (producteur)
    \item \textbf{Capacité Kafka} : > 10,000 msg/s
    \item \textbf{Capacité Spark} : > 1,000 transactions/s
    \item \textbf{Scalabilité} : Linéaire avec ajout de workers
\end{itemize}

\subsection{Exemple de Résultats}

\subsubsection{Statistiques Globales}
Après 3 minutes d'exécution (exemple) :

\begin{itemize}
    \item \textbf{Transactions analysées} : 180
    \item \textbf{Utilisateurs uniques} : 124
    \item \textbf{Chiffre d'affaires total} : 36,129.69 €
    \item \textbf{Montant moyen} : 259.93 €
    \item \textbf{Articles vendus} : 450
\end{itemize}

\subsubsection{Top 3 Catégories}
\begin{enumerate}
    \item \textbf{Electronics} : 8,500 € (35 transactions)
    \item \textbf{Fashion} : 7,200 € (28 transactions)
    \item \textbf{Home \& Garden} : 6,800 € (32 transactions)
\end{enumerate}

\subsubsection{Répartition Géographique}
\begin{itemize}
    \item Casablanca : 28\%
    \item Rabat : 19\%
    \item Marrakech : 18\%
    \item Fès : 15\%
    \item Tanger : 12\%
    \item Agadir : 8\%
\end{itemize}

\subsection{Visualisations}
Le dashboard fournit :

\begin{itemize}
    \item \textbf{4 cartes métriques} : Revenus, transactions, utilisateurs, montant moyen
    \item \textbf{Graphique en barres} : Revenus par catégorie
    \item \textbf{Graphique circulaire} : Revenus par ville
    \item \textbf{Graphique donut} : Méthodes de paiement
    \item \textbf{Tableau} : Top 10 transactions
\end{itemize}

\newpage

\section{Défis et Solutions}

\subsection{Défis Rencontrés}

\subsubsection{Permissions Docker}
\textbf{Problème :} Spark ne pouvait pas écrire dans le volume partagé

\textbf{Solution :} Création du répertoire avec permissions 777 avant le démarrage du consumer
\begin{lstlisting}[language=bash]
docker exec -u root spark-master mkdir -p /tmp/ecommerce-data/raw
docker exec -u root spark-master chmod -R 777 /tmp/ecommerce-data
\end{lstlisting}

\subsubsection{Partage de Données Dashboard-Spark}
\textbf{Problème :} Le dashboard ne pouvait pas accéder aux fichiers Parquet de Spark

\textbf{Solution :} Utilisation d'un volume Docker partagé \texttt{ecommerce\_data} monté sur les deux conteneurs

\subsubsection{Dépendances Dashboard}
\textbf{Problème :} PySpark nécessite Java, trop lourd pour le conteneur dashboard

\textbf{Solution :} Remplacement de PySpark par Pandas + PyArrow pour la lecture des fichiers Parquet

\subsection{Optimisations}

\subsubsection{Format Parquet}
Utilisation du format columnaire Parquet au lieu de CSV/JSON :
\begin{itemize}
    \item Réduction de 75\% de l'espace disque
    \item Amélioration de 10x des performances de lecture
    \item Compression Snappy intégrée
\end{itemize}

\subsubsection{Micro-batching Spark}
Configuration de fenêtres temporelles optimales :
\begin{itemize}
    \item Affichage console : 10 secondes
    \item Agrégations : 30 secondes
    \item Écriture Parquet : 30 secondes
\end{itemize}

\subsubsection{Caching Dashboard}
Utilisation de Pandas pour des lectures efficaces :
\begin{itemize}
    \item Lecture de tous les fichiers Parquet en une fois
    \item Agrégations en mémoire avec Pandas
    \item Pas de surcharge Spark pour chaque requête API
\end{itemize}

\newpage

\section{Évolutions Possibles}

\subsection{Améliorations Techniques}

\subsubsection{Scalabilité}
\begin{itemize}
    \item Ajout de workers Spark pour traiter plus de transactions/seconde
    \item Augmentation du nombre de partitions Kafka
    \item Réplication Kafka pour la haute disponibilité
    \item Cluster HDFS multi-nœuds
\end{itemize}

\subsubsection{Machine Learning}
\begin{itemize}
    \item Prédiction des ventes futures avec Spark MLlib
    \item Détection d'anomalies (fraudes)
    \item Recommandations produits
    \item Segmentation clients
\end{itemize}

\subsubsection{Monitoring}
\begin{itemize}
    \item Intégration Prometheus + Grafana
    \item Alertes sur métriques critiques
    \item Logs centralisés avec ELK Stack
    \item Traçabilité avec Jaeger
\end{itemize}

\subsection{Fonctionnalités Métier}

\subsubsection{Dashboard Avancé}
\begin{itemize}
    \item Filtres temporels (jour, semaine, mois)
    \item Comparaisons période à période
    \item Export de rapports PDF
    \item Alertes personnalisées
    \item Tableaux de bord multi-utilisateurs
\end{itemize}

\subsubsection{Analyses Avancées}
\begin{itemize}
    \item Analyse de cohortes
    \item Taux de conversion
    \item Panier moyen par segment
    \item Analyse RFM (Récence, Fréquence, Montant)
    \item Lifetime Value (LTV)
\end{itemize}

\subsection{Intégrations}

\begin{itemize}
    \item Connexion à des sources de données réelles (API e-commerce)
    \item Export vers data warehouse (Snowflake, BigQuery)
    \item Intégration avec outils BI (Tableau, Power BI)
    \item Notifications (Slack, Email)
\end{itemize}

\newpage

\section{Conclusion}

\subsection{Synthèse}
Ce projet a permis de concevoir et implémenter une plateforme complète d'analyse Big Data pour le traitement en temps réel de transactions e-commerce. L'architecture mise en place démontre la puissance des technologies modernes (Kafka, Spark, Docker) pour gérer des flux de données massifs.

\subsection{Compétences Acquises}

\subsubsection{Techniques}
\begin{itemize}
    \item Maîtrise de l'écosystème Apache (Kafka, Spark, Hadoop)
    \item Développement d'applications de streaming en temps réel
    \item Containerisation et orchestration avec Docker
    \item Développement web (Flask, JavaScript, Chart.js)
    \item Traitement de données avec Pandas et PySpark
    \item Scripting Bash pour l'automatisation
\end{itemize}

\subsubsection{Architecturales}
\begin{itemize}
    \item Conception d'architectures Big Data scalables
    \item Patterns de traitement streaming et batch
    \item Gestion de volumes Docker et réseaux
    \item Optimisation des performances
    \item Résolution de problèmes de production
\end{itemize}

\subsection{Résultats Atteints}
\begin{itemize}
    \item ✓ Pipeline de données fonctionnel end-to-end
    \item ✓ Traitement en temps réel avec latence < 15 secondes
    \item ✓ Dashboard interactif avec visualisations modernes
    \item ✓ Automatisation complète du déploiement
    \item ✓ Documentation technique exhaustive
    \item ✓ Scripts d'analyse et génération de rapports
\end{itemize}

\subsection{Perspectives}
Ce projet constitue une base solide pour des applications Big Data en production. Les évolutions possibles (ML, monitoring avancé, intégrations) permettraient de transformer cette plateforme en solution enterprise-grade pour l'analyse e-commerce.

L'expérience acquise sur ce projet est directement applicable dans l'industrie, où les architectures de streaming et l'analyse en temps réel sont devenues essentielles pour la compétitivité des entreprises.

\newpage

\section*{Annexes}

\subsection*{A. Structure du Projet}
\begin{lstlisting}
bigdata-project/
├── docker-compose.yml          # Configuration Docker
├── README.md                   # Documentation
├── requirements.txt            # Dependances Python
├── DASHBOARD_GUIDE.md          # Guide dashboard
├── scripts/
│   ├── producer.py             # Generateur transactions
│   ├── consumer_spark.py       # Consumer Spark Streaming
│   ├── analysis.py             # Analyse batch
│   ├── dashboard_server.py     # Backend Flask
│   └── automation/             # Scripts Bash
│       ├── setup.sh
│       ├── start-all.sh
│       ├── start-producer.sh
│       ├── start-consumer.sh
│       ├── run-analysis.sh
│       └── stop-all.sh
├── dashboard/
│   └── dashboard.html          # Interface web
└── data/                       # Donnees temporaires
\end{lstlisting}

\subsection*{B. Commandes Utiles}

\textbf{Démarrage complet :}
\begin{lstlisting}[language=bash]
./scripts/automation/setup.sh
./scripts/automation/start-all.sh
\end{lstlisting}

\textbf{Accès aux interfaces :}
\begin{itemize}
    \item Dashboard : \url{http://localhost:5000}
    \item Spark Master : \url{http://localhost:8080}
    \item Spark Application : \url{http://localhost:4040}
    \item HDFS NameNode : \url{http://localhost:9870}
\end{itemize}

\textbf{Arrêt :}
\begin{lstlisting}[language=bash]
./scripts/automation/stop-all.sh
\end{lstlisting}

\subsection*{C. Technologies et Versions}
\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Technologie} & \textbf{Version} \\
\hline
Apache Kafka & 7.5.0 (Confluent) \\
Apache Spark & 3.5.0 \\
Apache Hadoop & 3.2.1 \\
Apache Zookeeper & 7.5.0 (Confluent) \\
Python & 3.9 \\
Flask & 3.0.0 \\
Pandas & 2.1.4 \\
PyArrow & 14.0.1 \\
Chart.js & 4.4.0 \\
Docker & 20.10+ \\
Docker Compose & 2.0+ \\
\hline
\end{tabular}
\end{table}

\subsection*{D. Références}
\begin{itemize}
    \item Apache Kafka Documentation : \url{https://kafka.apache.org/documentation/}
    \item Apache Spark Streaming Guide : \url{https://spark.apache.org/docs/latest/streaming-programming-guide.html}
    \item Hadoop HDFS Architecture : \url{https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html}
    \item Flask Documentation : \url{https://flask.palletsprojects.com/}
    \item Chart.js Documentation : \url{https://www.chartjs.org/docs/}
\end{itemize}

\end{document}
