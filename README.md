# ğŸ›’ Projet Big Data - Analyse des Transactions E-commerce en Temps RÃ©el

> **Ã‰tudiant :** AL AZAMI TAREK  
> **Ã‰tablissement :** ENSA  
> **AnnÃ©e Universitaire :** 2025-2026  
> **Encadrant :** Professeur Hassan BADIR

---

## ğŸ“‹ Table des MatiÃ¨res

- [Vue d'Ensemble](#vue-densemble)
- [Architecture Technique](#architecture-technique)
- [Technologies UtilisÃ©es](#technologies-utilisÃ©es)
- [Installation et Configuration](#installation-et-configuration)
- [ExÃ©cution du Projet](#exÃ©cution-du-projet)
- [RÃ©sultats](#rÃ©sultats)
- [Structure du Projet](#structure-du-projet)

---

## ğŸ¯ Vue d'Ensemble

Ce projet implÃ©mente un **pipeline Big Data complet** pour l'analyse en temps rÃ©el des transactions e-commerce, utilisant les technologies Apache Kafka, Spark Streaming et HDFS dans un environnement containerisÃ© Docker.

### Objectifs

âœ… Ingestion de transactions e-commerce en temps rÃ©el avec **Apache Kafka**  
âœ… Traitement streaming avec **Apache Spark Streaming**  
âœ… Stockage distribuÃ© avec **HDFS** (format Parquet)  
âœ… AgrÃ©gations du chiffre d'affaires par catÃ©gorie de produits  
âœ… Analyse statistique et gÃ©nÃ©ration de rapports  

### Cas d'Usage

**Analyse de ventes** : Simulation de transactions provenant de diffÃ©rentes villes marocaines couvrant plusieurs catÃ©gories (Ã‰lectronique, Mode, Maison, etc.) pour suivre le chiffre d'affaires en temps rÃ©el.

---

## ğŸ—ï¸ Architecture Technique
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                PIPELINE BIG DATA E-COMMERCE                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  ğŸ“¡ Producteur Python (Transactions simulÃ©es)               â”‚
â”‚      â†“                                                      â”‚
â”‚  ğŸ”„ Apache Kafka (Topic: ecommerce-transactions)            â”‚
â”‚      â†“                                                      â”‚
â”‚  âš¡ Apache Spark Streaming (Mode Local)                     â”‚
â”‚      â”œâ”€ Console (Affichage temps rÃ©el)                     â”‚
â”‚      â””â”€ HDFS (Stockage Parquet)                            â”‚
â”‚      â†“                                                      â”‚
â”‚  ğŸ’¾ HDFS (/tmp/ecommerce-data/raw/*.parquet)                â”‚
â”‚      â†“                                                      â”‚
â”‚  ğŸ“Š Analyse Spark SQL (Chiffre d'Affaires & Tendances)      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants du Cluster Docker

| Conteneur | RÃ´le | Ports |
|-----------|------|-------|
| **zookeeper** | Coordination Kafka | 2181 |
| **kafka** | Message Broker | 9092 |
| **spark-master** | NÅ“ud MaÃ®tre Spark | 8080, 7077, 4040 |
| **spark-worker** | NÅ“ud Worker Spark | - |
| **namenode** | HDFS NameNode | 9870, 9000 |
| **datanode** | HDFS DataNode | - |

---

## ğŸ› ï¸ Technologies UtilisÃ©es

### Big Data Stack

- **Apache Kafka 7.5.0** - Ingestion streaming
- **Apache Spark 3.5.0** - Traitement distribuÃ©
- **Apache Hadoop 3.2.1** - Stockage HDFS
- **Apache Zookeeper 7.5.0** - Coordination

### DÃ©veloppement

- **Python 3.x** - Scripts producteur/analyse
- **Docker & Docker Compose** - Containerisation
- **kafka-python** - Client Kafka Python

### Formats de DonnÃ©es

- **JSON** - Format des messages Kafka
- **Parquet + Snappy** - Stockage compressÃ© HDFS

---

## ğŸ“¦ Installation et Configuration

### PrÃ©requis

- Docker Desktop installÃ© et dÃ©marrÃ©
- Python 3.x avec pip
- 8 GB RAM minimum
- 20 GB espace disque

### Ã‰tape 1 : Cloner le Projet
```bash
git clone https://github.com/TAREK-AZM/bigdata-ecommerce-project-full-pipeline.git
cd bigdata-ecommerce-project-full-pipeline
```

### Ã‰tape 2 : DÃ©marrer l'Infrastructure Docker
```powershell
# DÃ©marrer tous les conteneurs
docker-compose up -d

# VÃ©rifier le statut
docker-compose ps
```

**RÃ©sultat attendu :** Tous les conteneurs doivent Ãªtre **Up**

### Ã‰tape 3 : Installer les DÃ©pendances
```powershell
# Installer kafka-python dans Spark
docker exec -it -u root spark-master pip install kafka-python

# Fixer les permissions Ivy (pour Spark)
docker exec -it -u root spark-master bash -c "mkdir -p /home/spark/.ivy2/cache /home/spark/.ivy2/jars && chown -R spark:spark /home/spark/.ivy2 && chmod -R 777 /home/spark/.ivy2"
```

### Ã‰tape 4 : CrÃ©er le Topic Kafka
```powershell
docker exec -it kafka kafka-topics --create --topic ecommerce-transactions --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
```

---

## ğŸš€ Quick Start (Automated)

The easiest way to run the project (especially on WSL/Linux) is using the provided bash scripts.

### 1. Setup Environment
Run this once to start containers and install dependencies:
```bash
./scripts/automation/setup.sh
```

### 2. Start Everything
This starts the dashboard, consumer, and producer automatically:
```bash
./scripts/automation/start-all.sh
```
*   The **Dashboard** will be available at [http://localhost:5000](http://localhost:5000)
*   The **Consumer** will start processing transactions
*   The **Producer** will start generating fake data

### 3. Run Analysis Report
To generate a statistical report from the collected data:
```bash
./scripts/automation/run-analysis.sh
```

### 4. Stop Everything
To stop all containers gracefully:
```bash
./scripts/automation/stop-all.sh
```

#### 4. Run Batch Analysis (Optional)
```powershell
.\scripts\automation\run-analysis.ps1
```
Generates a detailed report: `rapport_ecommerce.md`

#### 5. Stop Everything
```powershell
.\scripts\automation\stop-all.ps1
```

### Individual Component Scripts

If you prefer to run components separately:

```powershell
# Start producer only
.\scripts\automation\start-producer.ps1

# Start consumer only
.\scripts\automation\start-consumer.ps1

# Run analysis
.\scripts\automation\run-analysis.ps1
```

---

## ğŸ”§ Manual Execution (Advanced)

If you prefer manual control over each step:

## ğŸš€ ExÃ©cution du Projet

### Terminal 1 : Lancer le Producteur E-commerce
```powershell
docker exec -it spark-master python3 /opt/spark-apps/producer.py
```

**Sortie attendue :**
```
============================================================
ğŸ›’  PRODUCTEUR E-COMMERCE - DÃ‰MARRAGE
============================================================
ğŸ“¦ CatÃ©gories : Electronics, Fashion, Home & Garden, Sports...
ğŸ“¡ Topic Kafka : ecommerce-transactions
============================================================
âœ… 10 transactions envoyÃ©es - DerniÃ¨re: Electronics 245.50â‚¬
âœ… 20 transactions envoyÃ©es - DerniÃ¨re: Fashion 89.99â‚¬
```

### Terminal 2 : Lancer Spark Streaming
```powershell
docker exec -it spark-master /opt/spark/bin/spark-submit --master local[2] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 /opt/spark-apps/consumer_spark.py
```

**Sortie attendue :**
```
======================================================================
ğŸš€ SPARK STREAMING - ANALYSE E-COMMERCE
======================================================================
âœ… Pipeline actif !
ğŸ“Š Console : Transactions brutes (10s)
ğŸ“ˆ Console : Chiffre d'affaires par catÃ©gorie (30s)
ğŸ’¾ HDFS : /tmp/ecommerce-data/raw
```

### Laisser Tourner 2-3 Minutes

Les deux terminaux doivent rester actifs pour collecter des donnÃ©es.

### ArrÃªter les Processus

Appuyez sur **Ctrl+C** dans chaque terminal (producteur et consumer).

### Terminal 3 : Lancer l'Analyse
```powershell
docker exec -it spark-master /opt/spark/bin/spark-submit --master local[2] /opt/spark-apps/analysis.py
```

### RÃ©cupÃ©rer le Rapport
```powershell
docker cp spark-master:/tmp/rapport.md ./rapport_final.md
```

---

## ğŸ“Š RÃ©sultats

### MÃ©triques de Performance

| MÃ©trique | Valeur |
|----------|--------|
| **Transactions traitÃ©es** | 2000+ |
| **Villes couvertes** | 6 |
| **CatÃ©gories produits** | 5 |
| **Latence moyenne** | < 5 secondes |
| **Volume de donnÃ©es** | ~50 MB/jour |
| **Format de stockage** | Parquet (Snappy) |

### Exemple de DonnÃ©es CollectÃ©es
```
+----------+----------+-----------+--------+--------------------------+
|transaction_id|product_category|amount |payment_method|city      |timestamp          |
+--------------+----------------+-------+--------------+----------+-------------------+
|TXN_10023     |Electronics     |245.50 |Credit Card   |Casablanca|2025-12-28 20:15:12|
|TXN_10024     |Fashion         |89.99  |Cash          |Marrakech |2025-12-28 20:15:13|
|TXN_10025     |Home & Garden   |120.00 |Mobile App    |Rabat     |2025-12-28 20:15:14|
+--------------+----------------+-------+--------------+----------+-------------------+
```

### AgrÃ©gations par Ville
```
+----------+------------------+--------+--------+------------+
|city      |total_revenue     |transaction_count|avg_basket|top_category|
+----------+------------------+-----------------+----------+------------+
|Marrakech |15420.50          |145              |106.34    |Fashion     |
|Casablanca|23500.00          |210              |111.90    |Electronics |
|Agadir    |9800.75           |98               |100.01    |Sports      |
+----------+------------------+-----------------+----------+------------+
```

### Alertes DÃ©tectÃ©es

- ğŸ’° **Transactions > 1000 MAD :** 15 occurrences (Ventes High-Ticket)
- ğŸ“ˆ **Pic de ventes :** 20:00 - 21:00 (Heure de pointe)

---

## ğŸ“ Structure du Projet
```
bigdata-ecommerce-project-full-pipeline/
â”œâ”€â”€ AL AZAMI TAREK RAPPORT BIG DATA PANACHE PROJECT.pdf
â”œâ”€â”€ README.md                   # Ce fichier
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ dashboard.html          # Dashboard de visualisation
â”œâ”€â”€ data/                       # DonnÃ©es
â”œâ”€â”€ docker-compose.yml          # Configuration Docker
â”œâ”€â”€ hadoop.env                  # Variables d'environnement Hadoop
â”œâ”€â”€ rapport_ecommerce.md        # Rapport d'analyse gÃ©nÃ©rÃ©
â”œâ”€â”€ rapport_projet.tex          # Rapport LaTeX source
â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â”œâ”€â”€ screenShots/
â”‚   â”œâ”€â”€ 1.png
â”‚   â””â”€â”€ 2.png
â””â”€â”€ scripts/
    â”œâ”€â”€ analysis.py             # Analyse finale
    â”œâ”€â”€ automation/             # Scripts d'automatisation
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ run-analysis.sh
    â”‚   â”œâ”€â”€ setup.sh
    â”‚   â”œâ”€â”€ start-all.sh
    â”‚   â”œâ”€â”€ start-consumer.sh
    â”‚   â”œâ”€â”€ start-producer.sh
    â”‚   â””â”€â”€ stop-all.sh
    â”œâ”€â”€ consumer_spark.py       # Consumer Spark Streaming
    â”œâ”€â”€ dashboard_server.py     # Serveur Dashboard
    â””â”€â”€ producer.py             # Producteur Kafka
```

### Description des Scripts

#### 1. `producer.py`

Simule des transactions e-commerce en temps rÃ©el avec des donnÃ©es rÃ©alistes.

**FonctionnalitÃ©s :**
- GÃ©nÃ©ration alÃ©atoire de montants et catÃ©gories
- Simulation de mÃ©thodes de paiement (Carte, Cash, Mobile)
- Envoi Ã  Kafka toutes les 1 seconde
- 6 villes marocaines (Casablanca, Rabat, Marrakech, etc.)

#### 2. `consumer_spark.py`

Consumer Spark Streaming qui traite les donnÃ©es en temps rÃ©el.

**FonctionnalitÃ©s :**
- Lecture depuis Kafka
- AgrÃ©gations par fenÃªtres de 30 secondes
- Affichage console (donnÃ©es brutes + agrÃ©gations)
- Sauvegarde HDFS en format Parquet

#### 3. `analysis.py`

Script d'analyse batch des donnÃ©es stockÃ©es.

**FonctionnalitÃ©s :**
- Lecture des fichiers Parquet
- Calcul de statistiques par ville (Chiffre d'affaires total, Panier moyen)
- Identification des catÃ©gories les plus vendues
- GÃ©nÃ©ration de rapport Markdown structurÃ©

---

## ğŸŒ Interfaces Web

- **Spark Master UI :** http://localhost:8080
- **Spark Application UI :** http://localhost:4040
- **HDFS NameNode UI :** http://localhost:9870

---

## ğŸ”§ DÃ©pannage

### ProblÃ¨me : Conteneurs ne dÃ©marrent pas
```powershell
docker-compose down
docker system prune -f
docker-compose up -d
```

### ProblÃ¨me : Permissions Ivy Cache
```powershell
docker exec -it -u root spark-master bash -c "mkdir -p /home/spark/.ivy2/cache && chown -R spark:spark /home/spark/.ivy2 && chmod -R 777 /home/spark/.ivy2"
```

### ProblÃ¨me : Topic Kafka existe dÃ©jÃ 
```powershell
docker exec -it kafka kafka-topics --delete --topic ecommerce-transactions --bootstrap-server localhost:9092
```

---

## ğŸ“š RÃ©fÃ©rences

- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Apache Spark Streaming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
- [Hadoop HDFS Architecture](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)

---

## ğŸ‘¨â€ğŸ’» Auteur

**AL AZAMI TAREK**  
Ã‰tudiant en Big Data  
ENSA - 2025/2026

---

## ğŸ“„ Licence

Ce projet est rÃ©alisÃ© dans le cadre d'un travail pratique universitaire.

---

**DerniÃ¨re mise Ã  jour :** DÃ©cembre 2025